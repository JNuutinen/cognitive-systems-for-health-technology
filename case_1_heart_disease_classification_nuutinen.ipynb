{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Case 1: Heart disease classification\n",
    "\n",
    "### Juha Nuutinen\n",
    "\n",
    "### 20.01.2019\n",
    "\n",
    "### Helsinki Metropolia University of Applied Sciences\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objectives\n",
    "This notebook documents the process of using neural networks to try and predict some kind of heart disease for an individual from a set of their biological attributes.\n",
    "\n",
    "The goals of this assignment are to learn to use Python for machine learning with neural networks (from the `keras` library), read data from external sources using the `pandas` library, visualize data with `matplotlib`, and document the results clearly.\n",
    "Learning to use the neural network includes testing of different model architectures (number of layers, number of units, activation functions), and solver optimizers and training settings (epochs, batch sizes, validation splits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data\n",
    "\n",
    "### Origin\n",
    "\n",
    "The data is provided by <a href=\"https://archive.ics.uci.edu/ml/index.php\">UC Irvine Machine Learning Repository</a>, and the data folder can be found <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\">here</a>. For this assignment, we are going to use the preprocessed data from Cleveland Clinic Foundation (`processed.cleveland.data`). The principal investigator responsible for the collection of the data is Robert Detrano, M.D., Ph.D. in Cleveland Clinic Foundation. The data is from the year 1988.\n",
    "\n",
    "### Description\n",
    "\n",
    "All information and numbers in this section are taken from the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names\">heart-disease.names</a> file. The data is in CSV (Comma Separated Values) format and it contains a total of 303 instances. Each instance has 14 attributes. The non-processed data contained originally a total of 76 attributes. Missing values are encoded with a question mark (?).\n",
    "\n",
    "A list of the attributes in each instance:\n",
    "1. age\n",
    "    * Age in years, numeric\n",
    "2. sex\n",
    "    * Nominal\n",
    "    * 1 = male\n",
    "    * 0 = female\n",
    "3. cp\n",
    "    * Chest pain type, nominal\n",
    "    * 1 = typical angina\n",
    "    * 2 = atypical angina\n",
    "    * 3 = non-anginal pain\n",
    "    * 4 = asymptomatic\n",
    "4. trestbps\n",
    "    * Resting blood pressure in mm Hg, numeric\n",
    "5. chol\n",
    "    * Serum cholestoral in mg/dl, numeric\n",
    "6. fbs\n",
    "    * Fasting blood sugar > 120 mg/dl, nominal\n",
    "    * 1 = true\n",
    "    * 0 = false\n",
    "7. restecg\n",
    "    * Resting electrocardiaographic results, nominal\n",
    "    * 0 = normal\n",
    "    * 1 = having ST-T wave abnormality (T  wave inversions and/or ST elevation or depression of > 0.05mV\n",
    "    * 2 = showing probable or definite left ventricular hypertrophy by  Estes' criteria\n",
    "8. thalach\n",
    "    * Maximum heart rate achieved, numeric\n",
    "9. exang\n",
    "    * Exercise induced angina, nominal\n",
    "    * 1 = yes\n",
    "    * 0 = no\n",
    "10. oldpeak\n",
    "    * St depression induced by exercise relative to rest, numeric\n",
    "11. slope\n",
    "    * The slope of the peak exercise relative to rest, nominal\n",
    "    * 1 = upslopping\n",
    "    * 2 = flat\n",
    "    * 3 = downslopping\n",
    "12. ca\n",
    "    * Number of major vessels (0-3) colored by flourosopy, numeric\n",
    "13. thal\n",
    "    * Nominal\n",
    "    * 3 = normal\n",
    "    * 6 = fixed defect\n",
    "    * 7 = reversable defect\n",
    "14. num (the predicted value)\n",
    "    * Diagnosis of heart disease (angiographic disease status), nominal\n",
    "    * 0 = absence of disease\n",
    "    * 1, 2, 3, 4 = presence of disease\n",
    "\n",
    "In this data set, there are a total of 164 disease-free instances (num = 0), and 139 instances with disease (num = 1, 2, 3 or 4), totaling to 303 instances.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "#### Read in the data\n",
    "The data is read into a pandas DataFrame in the cell below. Also the column names are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "dataframe = pd.read_csv(url, \n",
    "                        sep = ',', \n",
    "                        header = None, \n",
    "                        index_col = None,\n",
    "                        na_values = '?')\n",
    "\n",
    "# The CSV data does not contain a header row, so the column names\n",
    "# must be set manually.\n",
    "names = [\"age\", \"sex\", \"cp\",\"trestbps\", \"chol\", \"fbs\",\"restecg\",\n",
    "         \"thalac\",\"exang\",\"oldpeak\",\"slope\",\"ca\",\"thal\",\"num\"]\n",
    "\n",
    "dataframe.columns = names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set column (attribute) types\n",
    "The attributes 2 (sex), 3 (cp), 6 (fbs), 7 (restecg), 9 (exang), 11 (slope) 13 (thal) and 14 (num) are nominal, all other are numeric. Column types are converted appropriately in the cell below. We'll only need to change the type of the nominal columns to categorical, as all columns are by fedault interpreted as numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.astype({\"sex\": \"category\",\n",
    "                              \"cp\": \"category\",\n",
    "                              \"fbs\": \"category\",\n",
    "                              \"restecg\": \"category\",\n",
    "                              \"exang\": \"category\",\n",
    "                              \"slope\": \"category\",\n",
    "                              \"thal\": \"category\",\n",
    "                              \"num\": \"category\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill missing values\n",
    "Next the missing values are replaces with mean values for the corresponding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many missing values there are.\n",
    "# Missing values are '?' in the original data, but in the read_csv\n",
    "# line in an earlier cell, they were converted to NaNs, which are the default representation\n",
    "# for missing values in Pandas.\n",
    "\n",
    "dataframe = dataframe.fillna(dataframe.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the numeric values\n",
    "The next step is to normalize all the numeric values, to get them all to be in somewhat of the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "dataframe[[\"age\", \"trestbps\", \"chol\", \"thalac\", \"oldpeak\", \"ca\"]] \\\n",
    "    = scaler.fit_transform(dataframe[[\"age\", \"trestbps\", \"chol\", \"thalac\", \"oldpeak\", \"ca\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify the labels to binary\n",
    "Because we are only trying to predict the presence of a heart disease, and not the type, the labels need to be converted to a binary form. If the label is 0 (= no disease) leave it as-is. Otherwise set it to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"num\"] = dataframe[\"num\"].mask(dataframe[\"num\"] != 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the data\n",
    "Data is randomly shuffled, to get rid of any possible structure in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = shuffle(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the data to a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 212\n",
      "Size of validation set: 91\n"
     ]
    }
   ],
   "source": [
    "df_train, df_validate = np.split(dataframe.sample(frac=1),\n",
    "                                 [int(0.7*len(dataframe))])\n",
    "print(\"Size of training set: {0}\".format(len(df_train)))\n",
    "print(\"Size of validation set: {0}\".format(len(df_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling and compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(9, input_dim=13),\n",
    "    Activation(\"relu\"),\n",
    "    Dense(1),\n",
    "    Activation(\"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "212/212 [==============================] - 1s 3ms/step - loss: 0.9417 - acc: 0.3915\n",
      "Epoch 2/20\n",
      "212/212 [==============================] - 0s 283us/step - loss: 0.8583 - acc: 0.3726\n",
      "Epoch 3/20\n",
      "212/212 [==============================] - 0s 247us/step - loss: 0.8042 - acc: 0.3774\n",
      "Epoch 4/20\n",
      "212/212 [==============================] - 0s 285us/step - loss: 0.7627 - acc: 0.4151\n",
      "Epoch 5/20\n",
      "212/212 [==============================] - 0s 296us/step - loss: 0.7276 - acc: 0.4340\n",
      "Epoch 6/20\n",
      "212/212 [==============================] - 0s 240us/step - loss: 0.6975 - acc: 0.5189\n",
      "Epoch 7/20\n",
      "212/212 [==============================] - 0s 150us/step - loss: 0.6722 - acc: 0.5802\n",
      "Epoch 8/20\n",
      "212/212 [==============================] - 0s 219us/step - loss: 0.6500 - acc: 0.6321\n",
      "Epoch 9/20\n",
      "212/212 [==============================] - 0s 292us/step - loss: 0.6291 - acc: 0.7170\n",
      "Epoch 10/20\n",
      "212/212 [==============================] - 0s 204us/step - loss: 0.6112 - acc: 0.7453\n",
      "Epoch 11/20\n",
      "212/212 [==============================] - 0s 211us/step - loss: 0.5938 - acc: 0.7783\n",
      "Epoch 12/20\n",
      "212/212 [==============================] - 0s 210us/step - loss: 0.5772 - acc: 0.7877\n",
      "Epoch 13/20\n",
      "212/212 [==============================] - 0s 459us/step - loss: 0.5624 - acc: 0.8113\n",
      "Epoch 14/20\n",
      "212/212 [==============================] - 0s 394us/step - loss: 0.5482 - acc: 0.8019\n",
      "Epoch 15/20\n",
      "212/212 [==============================] - 0s 384us/step - loss: 0.5344 - acc: 0.8113\n",
      "Epoch 16/20\n",
      "212/212 [==============================] - 0s 324us/step - loss: 0.5211 - acc: 0.7925\n",
      "Epoch 17/20\n",
      "212/212 [==============================] - 0s 317us/step - loss: 0.5089 - acc: 0.8066\n",
      "Epoch 18/20\n",
      "212/212 [==============================] - 0s 350us/step - loss: 0.4971 - acc: 0.8066\n",
      "Epoch 19/20\n",
      "212/212 [==============================] - 0s 352us/step - loss: 0.4850 - acc: 0.8160\n",
      "Epoch 20/20\n",
      "212/212 [==============================] - 0s 240us/step - loss: 0.4742 - acc: 0.8019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd992a71d68>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = df_train.loc[:, 'age':'thal']\n",
    "training_labels = df_train.loc[:, 'num']\n",
    "model.fit(training_data, training_labels, epochs = 20, batch_size = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 0s 149us/step\n",
      "Validation set accuracy: 0.7142857214906714\n"
     ]
    }
   ],
   "source": [
    "validation_data = df_validate.loc[:, 'age':'thal']\n",
    "validation_labels = df_validate.loc[:, 'num']\n",
    "score = model.evaluate(validation_data, validation_labels, batch_size=12)\n",
    "print(\"Validation set accuracy: {0}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
